# -*- coding: utf-8 -*-
"""IMDB Review Sentiment Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/imdb-review-sentiment-analysis-9aa135cf-6101-4bd3-adab-0131deec34ed.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250214/auto/storage/goog4_request%26X-Goog-Date%3D20250214T163015Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8f5bb7de21504bc29d19a02f0892e1c83b19fdf1f500035feb16c1e45eae6ab15544f56a955992e9ec0f4e3094f72026f7b389f138061790c7633aafe56f74eb63f482bd8da962f5917ad8ed1b33427be2570536916920d3b5e18807252d0cf13313b7ca369d7197bbe613ece6c7e6a7529eef09cd6922eb27f191c56139127aa2e354592e69736379800c56dd5f77d3498ae0c40715336fb95a3f4436cd03b75513998a205837f805280e76bdd13609b0aa4d8c3ac14a212ea34594936aea9fc82bfd5a7e427dfe05d8bd2f5a8575ce1a05e99af7236ced5a432474bbca620089cf26dc479c1a8e188d8e2f08fa9caaa3603f29aace387f625f7a7b70913e35
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
lakshmi25npathi_imdb_dataset_of_50k_movie_reviews_path = kagglehub.dataset_download('lakshmi25npathi/imdb-dataset-of-50k-movie-reviews')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""# Import necessary libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from nltk import pos_tag, word_tokenize
from nltk.corpus import stopwords
import nltk
from gensim.models import Word2Vec
import re
import string
from wordcloud import WordCloud
import warnings
warnings.filterwarnings('ignore')

"""# Download NLTK"""

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

"""# Load the dataset"""

df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')

"""# Basic EDA"""

print("Dataset Shape:", df.shape)
print("\nColumns:", df.columns.tolist())
print("\nSample Data:")
display(df.head())

"""# Text preprocessing function"""

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)

    # Remove URLs
    text = re.sub(r'https?://\S+|www\.\S+', '', text)

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Remove numbers
    text = re.sub(r'\d+', '', text)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = text.split()
    text = ' '.join([word for word in words if word not in stop_words])

    # Remove extra whitespace
    text = ' '.join(text.split())

    return text

"""# Apply preprocessing"""

print("Preprocessing reviews...")
df['processed_review'] = df['review'].apply(preprocess_text)

"""# 1. Total words and vocabulary"""

def get_corpus_stats(texts):
    all_words = ' '.join(texts).split()
    vocabulary = set(all_words)
    return len(all_words), len(vocabulary)

total_words, vocab_size = get_corpus_stats(df['processed_review'])
print(f"\nTotal words in corpus: {total_words}")
print(f"Vocabulary size: {vocab_size}")

"""# 2. One Hot Encoding (for demonstration, using first 1000 most common words)"""

def create_one_hot_encoding(texts, max_features=1000):
    vectorizer = CountVectorizer(max_features=max_features, binary=True)
    one_hot = vectorizer.fit_transform(texts)
    return one_hot, vectorizer.get_feature_names_out()

one_hot_matrix, one_hot_features = create_one_hot_encoding(df['processed_review'])
print(f"\nOne-hot encoding shape: {one_hot_matrix.shape}")

"""# 3. Bag of Words"""

def create_bow(texts, max_features=5000):
    vectorizer = CountVectorizer(max_features=max_features)
    bow = vectorizer.fit_transform(texts)

    # Get word frequencies
    word_freq = pd.DataFrame(
        bow.sum(axis=0).T,
        index=vectorizer.get_feature_names_out(),
        columns=['frequency']
    ).sort_values('frequency', ascending=False)

    return bow, word_freq

bow_matrix, word_frequencies = create_bow(df['processed_review'])
print("\nTop 10 most frequent words:")
print(word_frequencies.head(10))

"""# 4. N-grams"""

def create_ngrams(texts, n, max_features=5000):
    vectorizer = CountVectorizer(ngram_range=(n,n), max_features=max_features)
    ngrams = vectorizer.fit_transform(texts)
    return ngrams, len(vectorizer.get_feature_names_out())

bigram_matrix, bigram_vocab_size = create_ngrams(df['processed_review'], 2)
trigram_matrix, trigram_vocab_size = create_ngrams(df['processed_review'], 3)

print(f"\nBigram vocabulary size: {bigram_vocab_size}")
print(f"Trigram vocabulary size: {trigram_vocab_size}")

"""# 5. TF-IDF"""

def analyze_tfidf(texts, max_features=5000):
    tfidf_vectorizer = TfidfVectorizer(max_features=max_features)
    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)

    # Calculate IDF scores
    idf_scores = pd.DataFrame(
        {'term': tfidf_vectorizer.get_feature_names_out(),
         'idf_score': tfidf_vectorizer.idf_}
    ).sort_values('idf_score', ascending=False)

    return tfidf_matrix, idf_scores

tfidf_matrix, idf_scores = analyze_tfidf(df['processed_review'])
print("\nTop 10 words by IDF score:")
print(idf_scores.head(10))

"""# 6. Additional EDA"""

def perform_eda(df):
    # Review length analysis
    df['review_length'] = df['processed_review'].str.len()

    # Plot review length distribution
    plt.figure(figsize=(10, 6))
    sns.histplot(data=df, x='review_length', bins=50)
    plt.title('Distribution of Processed Review Lengths')
    plt.xlabel('Review Length')
    plt.show()

    # Word cloud
    plt.figure(figsize=(12, 8))
    wordcloud = WordCloud(width=1200, height=800, background_color='white').generate(' '.join(df['processed_review']))
    plt.imshow(wordcloud)
    plt.axis('off')
    plt.title('Word Cloud of Processed Reviews')
    plt.show()

    # Sentiment distribution
    plt.figure(figsize=(8, 6))
    sns.countplot(data=df, x='sentiment')
    plt.title('Distribution of Sentiment in Reviews')
    plt.show()

perform_eda(df)

"""# 7. POS Tagging"""

def analyze_pos_tags(texts, sample_size=1000):
    # Apply POS tagging to a sample
    sample_texts = texts.head(sample_size)
    pos_tags = [pos_tag(word_tokenize(text)) for text in sample_texts]

    # Count POS tags
    tag_counts = Counter([tag for tags in pos_tags for _, tag in tags])

    # Plot distribution
    plt.figure(figsize=(12, 6))
    tags, counts = zip(*tag_counts.most_common(15))
    sns.barplot(x=list(tags), y=list(counts))
    plt.xticks(rotation=45)
    plt.title('Distribution of POS Tags')
    plt.show()

    return pos_tags

pos_tags = analyze_pos_tags(df['processed_review'])

"""# 8. Word2Vec"""

def train_word2vec(texts):
    # Tokenize sentences
    sentences = [text.split() for text in texts]

    # Train Word2Vec model
    model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)

    return model

w2v_model = train_word2vec(df['processed_review'])

"""# Additional visualizations"""

def plot_additional_insights(df, word_frequencies):
    # Word frequency distribution
    plt.figure(figsize=(12, 6))
    top_words = word_frequencies.head(20)
    sns.barplot(x=top_words.index, y='frequency', data=top_words)
    plt.xticks(rotation=45)
    plt.title('Top 20 Most Frequent Words')
    plt.show()

    # Average review length by sentiment
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df, x='sentiment', y='review_length')
    plt.title('Review Length Distribution by Sentiment')
    plt.show()

plot_additional_insights(df, word_frequencies)